{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42fd1a9c",
   "metadata": {},
   "source": [
    "# StockTwits Data Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e90c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time, json, os, traceback\n",
    "from json import JSONDecodeError\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from time import sleep\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTwitsAPIScraper:\n",
    "    def __init__(self, symbol, date, maxId):\n",
    "        self.symbol = symbol\n",
    "        self.link = \"https://api.stocktwits.com/api/2/streams/symbol/{}.json?\".format(symbol)\n",
    "        self.targetDate = date\n",
    "        self.tweets = []\n",
    "        self.reqeustQueue = deque()\n",
    "        self.maxId = maxId\n",
    "        self.initDir()\n",
    "\n",
    "    def setLimits(self, size, duration):\n",
    "        self.size = size\n",
    "        self.duration = duration\n",
    "        self.requestInterval = duration // size + 1 if duration % size else duration // size\n",
    "\n",
    "    # create directions if they don't exist\n",
    "    def initDir(self):\n",
    "        if not os.path.isdir(\"stocks\"):\n",
    "            os.mkdir(\"stocks\")\n",
    "        if not os.path.isdir(\"stocks/{}\".format(self.symbol)):\n",
    "            os.mkdir(\"stocks/{}\".format(self.symbol))\n",
    "\n",
    "    # write tweets we get and the ID of the last tweet in case system break down\n",
    "    def writeJson(self):\n",
    "        if self.tweets:\n",
    "            self.maxId = self.tweets[-1][\"id\"]\n",
    "            fileName = \"stocks/{}/{}.json\".format(self.symbol, self.maxId)\n",
    "            with open(fileName, \"w\") as f:\n",
    "                json.dump(self.tweets, f)\n",
    "    \n",
    "    def getCurrentUrl(self):\n",
    "        return self.link + \"max={}\".format(self.maxId)\n",
    "\n",
    "    # request manager\n",
    "    # can't exceed 200 requests within an hour\n",
    "    def requestManager(self):\n",
    "        if len(self.reqeustQueue) == self.size:\n",
    "            now = datetime.now()\n",
    "            firstRequest = self.reqeustQueue.popleft()\n",
    "            if now < firstRequest + timedelta(seconds=self.duration):\n",
    "                timeDiff = firstRequest - now\n",
    "                waitTime = timeDiff.total_seconds() + 1 + self.duration                \n",
    "                print(\"Reach request limit, wait for {} seconds.\".format(waitTime))\n",
    "                sleep(waitTime)\n",
    "\n",
    "    def getMessages(self, url):\n",
    "        self.requestManager()\n",
    "\n",
    "        response = requests.get(url)\n",
    "        self.reqeustQueue.append(datetime.now())\n",
    "        try:\n",
    "            data = json.loads(response.text)\n",
    "        except JSONDecodeError:\n",
    "            if \"Bad Gateway\" in response.text:\n",
    "                print(\"Just a Bad Gateway, wait for 1 minute.\")\n",
    "                sleep(60)\n",
    "                return True\n",
    "            print(len(self.reqeustQueue))\n",
    "            print(self.reqeustQueue[0], datetime.now())\n",
    "            print(url)\n",
    "            print(response.text)\n",
    "            print(traceback.format_exc())\n",
    "            raise Exception(\"Something worong with the response.\")\n",
    "        if data and data[\"response\"][\"status\"] == 200:\n",
    "            data[\"cursor\"][\"max\"]\n",
    "            for m in data[\"messages\"]:\n",
    "                record = {}            \n",
    "                createdAt = datetime.strptime(m[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                if createdAt < self.targetDate:\n",
    "                    return False\n",
    "                record[\"id\"] = m[\"id\"]\n",
    "                record[\"text\"] = m[\"body\"]\n",
    "                record[\"time\"] = m[\"created_at\"]\n",
    "#                 record[\"time\"] = createdAt.timestamp() # 改为po贴时间\n",
    "                record[\"sentiment\"] = m[\"entities\"][\"sentiment\"][\"basic\"] if m[\"entities\"][\"sentiment\"] else \"NONE\"\n",
    "                self.tweets.append(record)\n",
    "        else:\n",
    "            print(response.text)        \n",
    "        return True\n",
    "\n",
    "    def getTweetsAndWriteToFile(self):        \n",
    "        if not self.getMessages(self.getCurrentUrl()):\n",
    "            return False\n",
    "        self.writeJson()\n",
    "        print(\"Scrap {} tweets starting from {}.\".format(len(self.tweets), self.maxId))\n",
    "        self.tweets.clear()\n",
    "        sleep(self.requestInterval)\n",
    "        return True\n",
    "\n",
    "    def scrapTweets(self):        \n",
    "        try:\n",
    "            doScrap = True\n",
    "            while doScrap:\n",
    "                doScrap = self.getTweetsAndWriteToFile()\n",
    "        except Exception:\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "symbol = input(\"Enter stock symbol: \")\n",
    "print(\"This scraper scraps tweets backward.\\n\\\n",
    "The ID you put in belongs the most recent tweet you're goint go scrap.\\n\\\n",
    "And the scraper will keep going backward to scrap older tweets.\")\n",
    "maxId = input(\"Enter the starting tweet ID: \") # 在user_name时间的位置上\n",
    "targetDate = input(\"Enter the earlest date (mmddyyyy): \") # 直接更改为具体时间\n",
    "print(\"You can only send 200 requests to StockTwits in an hour.\")\n",
    "requestLimit = input(\"Enter the limit of number of requests within an hour: \")\n",
    "\n",
    "scraper = StockTwitsAPIScraper(symbol, datetime.strptime(targetDate, \"%m%d%Y\"), int(maxId))\n",
    "scraper.setLimits(int(requestLimit), 3600)\n",
    "scraper.scrapTweets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
